{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TL;DR\n",
    "\n",
    "Both of these options work, try them both:\n",
    "- Train on X, evaluate on X\n",
    "- Train on X, evaluate on X_val\n",
    "\n",
    "CV doesn't seem to be able to take into account overfitting caused by excessive hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xm5ZaJ_qkvY"
   },
   "source": [
    "# Creating Validation Set\n",
    "\n",
    "It will be much faster to train the models if I can create a validation set that represents the test set. I want a validation set where if a model has a better score on it, the model will have a better score on the submissions.\n",
    "\n",
    "I will aim to create a simple hold-out validation set for maximum speed (this has a high chance of working given that all the folds in cross-validation seem to give the same results anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:36:35.731129Z",
     "start_time": "2021-02-08T13:36:34.563212Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34805,
     "status": "ok",
     "timestamp": 1612368991534,
     "user": {
      "displayName": "Adam Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhrEUWR1gM_kMZc1JaOXZTsyHAIY7wGzIWe3pircVE=s64",
      "userId": "08787610892899884155"
     },
     "user_tz": 0
    },
    "id": "2qKb4IH1qiih",
    "outputId": "4f1b9920-8083-4694-e1ce-29ce24daba03"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMClassifier\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV \n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "############ USE FOR GOOGLE COLAB ############\n",
    "# DATA_DIR = Path('/content/drive/MyDrive/Work/Delivery/Current/Earthquake_damage/data')\n",
    "# SUBMISSIONS_DIR = Path('drive/MyDrive/Work/Delivery/Current/Earthquake_damage/submissions')\n",
    "# MODEL_DIR = Path('/content/drive/MyDrive/Work/Delivery/Current/Earthquake_damage/models')\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#############################################\n",
    "\n",
    "\n",
    "### USE FOR LOCAL JUPYTER NOTEBOOKS ###\n",
    "DATA_DIR = Path('data')\n",
    "SUBMISSIONS_DIR = Path('submissions')\n",
    "MODEL_DIR = Path('models')\n",
    "#######################################\n",
    "\n",
    "# The code runs the same if working on Jupyter or Colab, just need to change the \n",
    "# dirs above\n",
    "\n",
    "X = pd.read_csv(DATA_DIR / 'train_values.csv', index_col='building_id')\n",
    "\n",
    "categorical_columns = X.select_dtypes(include='object').columns\n",
    "bool_columns = [col for col in X.columns if col.startswith('has')]\n",
    "X[categorical_columns] = X[categorical_columns].astype('category')\n",
    "X[bool_columns] = X[bool_columns].astype('bool')\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "y = pd.read_csv(DATA_DIR / 'train_labels.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:36:35.863712Z",
     "start_time": "2021-02-08T13:36:35.861315Z"
    },
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1612369011594,
     "user": {
      "displayName": "Adam Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhrEUWR1gM_kMZc1JaOXZTsyHAIY7wGzIWe3pircVE=s64",
      "userId": "08787610892899884155"
     },
     "user_tz": 0
    },
    "id": "Ixqzi1QxrV1H"
   },
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:36:37.537467Z",
     "start_time": "2021-02-08T13:36:36.590484Z"
    },
    "executionInfo": {
     "elapsed": 1977,
     "status": "ok",
     "timestamp": 1612369239753,
     "user": {
      "displayName": "Adam Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhrEUWR1gM_kMZc1JaOXZTsyHAIY7wGzIWe3pircVE=s64",
      "userId": "08787610892899884155"
     },
     "user_tz": 0
    },
    "id": "5HNp326nrqCQ"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33,\n",
    "                                                  random_state=42, stratify=y,\n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:35:34.899370Z",
     "start_time": "2021-02-08T13:35:34.884857Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1612370467159,
     "user": {
      "displayName": "Adam Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhrEUWR1gM_kMZc1JaOXZTsyHAIY7wGzIWe3pircVE=s64",
      "userId": "08787610892899884155"
     },
     "user_tz": 0
    },
    "id": "jcLnSf11wevx",
    "outputId": "ee3435fa-c781-413e-d87a-ee02cc0739eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "damage_grade\n",
       "1               0.096408\n",
       "2               0.568914\n",
       "3               0.334678\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts(normalize=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:35:34.908365Z",
     "start_time": "2021-02-08T13:35:34.900741Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1244,
     "status": "ok",
     "timestamp": 1612370481045,
     "user": {
      "displayName": "Adam Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhrEUWR1gM_kMZc1JaOXZTsyHAIY7wGzIWe3pircVE=s64",
      "userId": "08787610892899884155"
     },
     "user_tz": 0
    },
    "id": "MabJc7BzxOhp",
    "outputId": "c973f6e7-8013-47cc-cd79-26ad3a53d658"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "damage_grade\n",
       "1               0.096408\n",
       "2               0.568911\n",
       "3               0.334681\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p0bwjUvxTt_"
   },
   "source": [
    "Very similar distribution of data. Good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:36:40.113390Z",
     "start_time": "2021-02-08T13:36:40.109754Z"
    },
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1612370227878,
     "user": {
      "displayName": "Adam Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhrEUWR1gM_kMZc1JaOXZTsyHAIY7wGzIWe3pircVE=s64",
      "userId": "08787610892899884155"
     },
     "user_tz": 0
    },
    "id": "8dc2Np0bwMF8"
   },
   "outputs": [],
   "source": [
    "top_14_features = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', \n",
    "                    'count_floors_pre_eq', 'age'\t, 'area_percentage'\t, \n",
    "                    'height_percentage', \n",
    "                    'has_superstructure_mud_mortar_stone',\n",
    "                    'has_superstructure_stone_flag', \n",
    "                    'has_superstructure_mud_mortar_brick',\n",
    "                    'has_superstructure_cement_mortar_brick',\n",
    "                    'has_superstructure_timber', 'count_families',\n",
    "                    'other_floor_type_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:36:40.437845Z",
     "start_time": "2021-02-08T13:36:40.434683Z"
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1612370255398,
     "user": {
      "displayName": "Adam Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhrEUWR1gM_kMZc1JaOXZTsyHAIY7wGzIWe3pircVE=s64",
      "userId": "08787610892899884155"
     },
     "user_tz": 0
    },
    "id": "JqDSbKIusXFm"
   },
   "outputs": [],
   "source": [
    "def calc_f1_score_X_val(model):\n",
    "    y_pred = model.predict(X_val[top_14_features])\n",
    "    return f1_score(y_val, y_pred, average='micro')\n",
    "\n",
    "def calc_f1_score_X(model):\n",
    "    y_pred = model.predict(X[top_14_features])\n",
    "    return f1_score(y, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:36:41.537959Z",
     "start_time": "2021-02-08T13:36:41.532867Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_clfs_dict():\n",
    "    \"\"\"\n",
    "    Quickly initialize untrained versions of the classifiers (and their\n",
    "    submission scores) in dict form to aid in model evaluation\n",
    "    \"\"\"\n",
    "    LGBM_01_30_SUBMISSION_SCORE = 0.7397\n",
    "    LGBM_02_02_SUBMISSION_SCORE = 0.7407\n",
    "    LGBM_02_08_SUBMISSION_SCORE = 0.7335\n",
    "\n",
    "    lgbm_01_30 = LGBMClassifier(boosting_type='goss', \n",
    "                                learning_rate=0.2, \n",
    "                                min_child_samples=40, \n",
    "                                n_estimators=330, \n",
    "                                num_leaves=90,\n",
    "                               random_state=42)\n",
    "\n",
    "    lgbm_02_02 = LGBMClassifier(boosting_type='goss', \n",
    "                                learning_rate=0.2, \n",
    "                                min_child_samples=40, \n",
    "                                n_estimators=240, \n",
    "                                num_leaves=120,\n",
    "                                random_state=42)\n",
    "\n",
    "    lgbm_02_08 = LGBMClassifier(boosting_type='goss',\n",
    "                               learning_rate=0.1,\n",
    "                               min_child_samples=70,\n",
    "                               n_estimators=400,\n",
    "                               num_leaves=120,\n",
    "                               random_state=42)\n",
    "    \n",
    "    clf_dict = {'lgbm_01_30': [lgbm_01_30, LGBM_01_30_SUBMISSION_SCORE], \n",
    "                'lgbm_02_02': [lgbm_02_02, LGBM_02_02_SUBMISSION_SCORE],\n",
    "                'lgbm_02_08': [lgbm_02_08, LGBM_02_08_SUBMISSION_SCORE]}\n",
    "    \n",
    "    return clf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T12:42:27.308709Z",
     "start_time": "2021-02-08T12:41:44.476861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_SCORES - FIT_PREDICT USING X_TRAIN AND X_VAL\n",
      "lgbm_01_30 val score (on X_val): 0.7244502843056314\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7247526134024814\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.7334736450423841\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs = init_clfs_dict()\n",
    "\n",
    "print('F1_SCORES - FIT_PREDICT USING X_TRAIN AND X_VAL')\n",
    "\n",
    "for clf_name, clf_and_submission_score in clfs.items():\n",
    "    clf = clf_and_submission_score[0]\n",
    "    submission_score = clf_and_submission_score[1]\n",
    "    \n",
    "    clf.fit(X_train[top_14_features], np.ravel(y_train), verbose=10)\n",
    "    y_pred = clf.predict(X_val[top_14_features])\n",
    "    val_score = f1_score(y_val, y_pred, average='micro')\n",
    "    \n",
    "    print(f'{clf_name} val score (on X_val):'  , val_score)\n",
    "    print(f'{clf_name} submission score:    ', submission_score, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly `lgbm_02_08` is overfitting the data (we could tell this perhaps from the huge number of estimators it is using). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T12:42:55.259841Z",
     "start_time": "2021-02-08T12:42:27.310269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_SCORES - PREDICT USING X\n",
      "lgbm_01_30 val score (on X): 0.7761942586559529\n",
      "lgbm_01_30 submission score: 0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X): 0.776355424576268\n",
      "lgbm_02_02 submission score: 0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X): 0.776186584088319\n",
      "lgbm_02_08 submission score: 0.7335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('F1_SCORES - PREDICT USING X')\n",
    "\n",
    "for clf_name, clf_and_submission_score in clfs.items():\n",
    "    clf = clf_and_submission_score[0]\n",
    "    submission_score = clf_and_submission_score[1]\n",
    "    \n",
    "    y_pred = clf.predict(X[top_14_features])\n",
    "    X_val_score = f1_score(y, y_pred, average='micro')\n",
    "\n",
    "    print(f'{clf_name} val score (on X):', X_val_score)\n",
    "    print(f'{clf_name} submission score:', submission_score, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T14:19:25.666414Z",
     "start_time": "2021-02-05T14:19:25.662355Z"
    }
   },
   "source": [
    "This is good! \n",
    "\n",
    "If we train on `X_train` and evaluate on on X, this seems to be true indication...\n",
    "\n",
    "What if we train on X and evaluate on X_val or train on X and evaluate on X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:38:33.224110Z",
     "start_time": "2021-02-08T13:37:49.282862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN ON X, EVALUATE ON X_VAL\n",
      "lgbm_01_30 val score (on X_val): 0.7890673147362179\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7891370829893372\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.785823090966174\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on X, evaluate on X_val\n",
    "clfs = init_clfs_dict()\n",
    "\n",
    "print('Train on X, evaluate on X_val'.upper())\n",
    "\n",
    "for clf_name, clf_and_submission_score in clfs.items():\n",
    "    clf = clf_and_submission_score[0]\n",
    "    submission_score = clf_and_submission_score[1]\n",
    "    \n",
    "    clf.fit(X[top_14_features], np.ravel(y), verbose=10)\n",
    "    \n",
    "    y_pred = clf.predict(X_val[top_14_features])\n",
    "    val_score = f1_score(y_val, y_pred, average='micro')\n",
    "    \n",
    "    print(f'{clf_name} val score (on X_val):'  , val_score)\n",
    "    print(f'{clf_name} submission score:    ', submission_score, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOOD! Now we see that the score on X_val is lower for `lgbm_02_08` than for the other ones and we also see that the eval scores match up perfectly with the submission scores (though they are skewed slightly up). But I know that anything above 0.789 will be an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:39:38.323429Z",
     "start_time": "2021-02-08T13:38:39.805379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN ON X, EVALUATE ON X\n",
      "lgbm_01_30 val score (on X_val): 0.7870499345743109\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7879785572580305\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.7849279166234973\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on X and evaluate on X\n",
    "clfs = init_clfs_dict()\n",
    "\n",
    "print('Train on X, evaluate on X'.upper())\n",
    "\n",
    "for clf_name, clf_and_submission_score in clfs.items():\n",
    "    clf = clf_and_submission_score[0]\n",
    "    submission_score = clf_and_submission_score[1]\n",
    "    \n",
    "    clf.fit(X[top_14_features], np.ravel(y), verbose=10)\n",
    "    \n",
    "    y_pred = clf.predict(X[top_14_features])\n",
    "    val_score = f1_score(y, y_pred, average='micro')\n",
    "    \n",
    "    print(f'{clf_name} val score (on X_val):'  , val_score)\n",
    "    print(f'{clf_name} submission score:    ', submission_score, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also works fine too. If we train on X and evaluate on X, we can see a clear improvement. Weird that if we train on X_train and evaluate X_val it is different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:07.633106Z",
     "start_time": "2021-02-08T14:05:30.871147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_SCORES - 5 FOLD STRATIFIED CROSS-VALIDATION\n",
      "lgbm_01_30 f1_score        : 0.7255344373046112 (0.0029)\n",
      "lgbm_01_30 submission score: 0.7397 \n",
      "\n",
      "lgbm_02_02 f1_score        : 0.7257838562617362 (0.0021)\n",
      "lgbm_02_02 submission score: 0.7407 \n",
      "\n",
      "lgbm_02_08 f1_score        : 0.7339150593305502 (0.0038)\n",
      "lgbm_02_08 submission score: 0.7335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs = init_clfs_dict()\n",
    "\n",
    "print('F1_SCORES - 5 FOLD STRATIFIED CROSS-VALIDATION')\n",
    "\n",
    "for clf_name, clf_and_submission_score in clfs.items():\n",
    "    clf = clf_and_submission_score[0]\n",
    "    submission_score = clf_and_submission_score[1]\n",
    "    \n",
    "#     kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    results = cross_val_score(clf, X[top_14_features], np.ravel(y),\n",
    "                             cv=5, scoring='f1_micro', n_jobs=-1,\n",
    "                             verbose=0)\n",
    "\n",
    "    print(f'{clf_name} f1_score        : {results.mean()} ({results.std():.4f})')\n",
    "    print(f'{clf_name} submission score:', submission_score, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratifed KFold Cross-Validation is not a good predictor of model performance for this dataset or this problem. It does not pick up that `lgbm_02_08` overfits the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5kpwBuDzJMT"
   },
   "source": [
    "## Using Different train_test_split percentages\n",
    "\n",
    "See if using different sizes of validation splits with `train_test_split` leads to results in line with the leaderboard. It does not. \n",
    "\n",
    "None of the validation split sizes give results that reflect the leaderboard. Indeed, they often perfectly reflect the reverse leaderboard scores... how is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:44:09.195511Z",
     "start_time": "2021-02-08T13:44:09.192912Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val_split(val_size=0.33):\n",
    "    return train_test_split(X, y, test_size=val_size,\n",
    "                            random_state=42, stratify=y,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:48:43.788264Z",
     "start_time": "2021-02-08T13:48:43.784293Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_eval_custom_val_split_size(val_size):\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_val_split(val_size)\n",
    "    \n",
    "    clfs = init_clfs_dict()\n",
    "\n",
    "    print(f'Validation split: {val_size}')\n",
    "\n",
    "    for clf_name, clf_and_submission_score in clfs.items():\n",
    "        clf = clf_and_submission_score[0]\n",
    "        submission_score = clf_and_submission_score[1]\n",
    "\n",
    "        clf.fit(X_train[top_14_features], np.ravel(y_train), verbose=10)\n",
    "\n",
    "        y_pred = clf.predict(X_val[top_14_features])\n",
    "        val_score = f1_score(y_val, y_pred, average='micro')\n",
    "\n",
    "        print(f'{clf_name} val score (on X_val):'  , val_score)\n",
    "        print(f'{clf_name} submission score:    ', submission_score, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:53:38.530627Z",
     "start_time": "2021-02-08T13:49:23.504995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation split: 0.1\n",
      "lgbm_01_30 val score (on X_val): 0.7275238862668356\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7272936571889029\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.7363493342542495\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n",
      "Validation split: 0.2\n",
      "lgbm_01_30 val score (on X_val): 0.7269814470175169\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7261372575353504\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.7359605533278333\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n",
      "Validation split: 0.25\n",
      "lgbm_01_30 val score (on X_val): 0.7272797040720789\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7237954904759712\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.7369802458903163\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n",
      "Validation split: 0.3\n",
      "lgbm_01_30 val score (on X_val): 0.7256494544710351\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7245750246223507\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.733272790064082\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n",
      "Validation split: 0.33\n",
      "lgbm_01_30 val score (on X_val): 0.7244502843056314\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7247526134024814\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.7334736450423841\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n",
      "Validation split: 0.4\n",
      "lgbm_01_30 val score (on X_val): 0.7210694448441592\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7217985245728648\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.7305858539346323\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n",
      "Validation split: 0.5\n",
      "lgbm_01_30 val score (on X_val): 0.7150060245124751\n",
      "lgbm_01_30 submission score:     0.7397 \n",
      "\n",
      "lgbm_02_02 val score (on X_val): 0.7153590532689696\n",
      "lgbm_02_02 submission score:     0.7407 \n",
      "\n",
      "lgbm_02_08 val score (on X_val): 0.7257196798182669\n",
      "lgbm_02_08 submission score:     0.7335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_splits = [0.1, 0.2, 0.25, 0.3, 0.33, 0.4, 0.5]\n",
    "for val_split in val_splits:\n",
    "    train_and_eval_custom_val_split_size(val_split)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPFeiygnSDGSZuGi70gjF2A",
   "collapsed_sections": [],
   "name": "Creating Validation Set.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
