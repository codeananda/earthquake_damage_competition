{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tough-museum",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Let's implement some bagged LGBM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "anonymous-savannah",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T10:55:19.011152Z",
     "start_time": "2021-03-09T10:55:16.247784Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "### USE FOR LOCAL JUPYTER NOTEBOOKS ###\n",
    "DOWNLOAD_DIR = Path('../download')\n",
    "DATA_DIR = Path('../data')\n",
    "SUBMISSIONS_DIR = Path('../submissions')\n",
    "MODEL_DIR = Path('../models')\n",
    "#######################################\n",
    "\n",
    "X = pd.read_csv(DOWNLOAD_DIR / 'train_values.csv', index_col='building_id')\n",
    "categorical_columns = X.select_dtypes(include='object').columns\n",
    "bool_columns = [col for col in X.columns if col.startswith('has')]\n",
    "\n",
    "X_test = pd.read_csv(DOWNLOAD_DIR / 'test_values.csv', index_col='building_id')\n",
    "y = pd.read_csv(DOWNLOAD_DIR / 'train_labels.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "determined-library",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T10:55:21.230554Z",
     "start_time": "2021-03-09T10:55:19.170516Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtheadammurphy\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unexpected-desire",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T10:55:48.232889Z",
     "start_time": "2021-03-09T10:55:37.445894Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.21 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">efficient-mountain-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/theadammurphy/earthquake_damage_competition\" target=\"_blank\">https://wandb.ai/theadammurphy/earthquake_damage_competition</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/theadammurphy/earthquake_damage_competition/runs/2ol1e3kn\" target=\"_blank\">https://wandb.ai/theadammurphy/earthquake_damage_competition/runs/2ol1e3kn</a><br/>\n",
       "                Run data is saved locally in <code>/Users/king/Google Drive/Work/Delivery/Current/earthquake_damage_competition/analysis/wandb/run-20210309_125537-2ol1e3kn</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2ol1e3kn)</h1><iframe src=\"https://wandb.ai/theadammurphy/earthquake_damage_competition/runs/2ol1e3kn\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1351fafd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='earthquake_damage_competition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "documentary-victoria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T10:45:38.235198Z",
     "start_time": "2021-03-09T10:45:38.228806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stock-companion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T10:59:55.881609Z",
     "start_time": "2021-03-09T10:59:55.837410Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "\n",
    "t = [('ord_encoder', OrdinalEncoder(dtype=int), categorical_columns)]\n",
    "ct = ColumnTransformer(transformers=t, remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brief-karma",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:00:54.343115Z",
     "start_time": "2021-03-09T11:00:53.907292Z"
    }
   },
   "outputs": [],
   "source": [
    "X_all_ints = ct.fit_transform(X)\n",
    "y = label_enc.fit_transform(np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "possible-neighbor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:01:03.540130Z",
     "start_time": "2021-03-09T11:01:03.497668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note that append for pandas objects works differently to append with\n",
    "# python objects e.g. python append modifes the list in-place\n",
    "# pandas append returns a new object, leaving the original unmodified\n",
    "not_categorical_columns = X.select_dtypes(exclude='object').columns\n",
    "cols_ordered_after_ordinal_encoding = categorical_columns.append(not_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confidential-elimination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:01:16.290538Z",
     "start_time": "2021-03-09T11:01:16.287044Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_cols = pd.Index(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'])\n",
    "cat_cols_plus_geo = categorical_columns.append(geo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adaptive-dietary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:02:01.704184Z",
     "start_time": "2021-03-09T11:02:01.699934Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_all_ints,\n",
    "                        label=y,\n",
    "                        feature_name=list(cols_ordered_after_ordinal_encoding),\n",
    "                        categorical_feature=list(cat_cols_plus_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "judicial-error",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:02:11.122654Z",
     "start_time": "2021-03-09T11:02:11.115303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taken from the docs for lgb.train and lgb.cv\n",
    "# Helpful Stackoverflow answer: \n",
    "# https://stackoverflow.com/questions/50931168/f1-score-metric-in-lightgbm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_ith_pred(preds, i, num_data, num_class):\n",
    "    \"\"\"\n",
    "    preds: 1D NumPY array\n",
    "        A 1D numpy array containing predicted probabilities. Has shape\n",
    "        (num_data * num_class,). So, For binary classification with \n",
    "        100 rows of data in your training set, preds is shape (200,), \n",
    "        i.e. (100 * 2,).\n",
    "    i: int\n",
    "        The row/sample in your training data you wish to calculate\n",
    "        the prediction for.\n",
    "    num_data: int\n",
    "        The number of rows/samples in your training data\n",
    "    num_class: int\n",
    "        The number of classes in your classification task.\n",
    "        Must be greater than 2.\n",
    "    \n",
    "    \n",
    "    LightGBM docs tell us that to get the probability of class 0 for \n",
    "    the 5th row of the dataset we do preds[0 * num_data + 5].\n",
    "    For class 1 prediction of 7th row, do preds[1 * num_data + 7].\n",
    "    \n",
    "    sklearn's f1_score(y_true, y_pred) expects y_pred to be of the form\n",
    "    [0, 1, 1, 1, 1, 0...] and not probabilities.\n",
    "    \n",
    "    This function translates preds into the form sklearn's f1_score \n",
    "    understands.\n",
    "    \"\"\"\n",
    "    # Does not work for binary classification, preds has a different form\n",
    "    # in that case\n",
    "    assert num_classs > 2\n",
    "    \n",
    "    preds_for_ith_row = [preds[class_label * num_data + i]\n",
    "                        for class_label in range(num_class)]\n",
    "    \n",
    "    # The element with the highest probability is predicted\n",
    "    return np.argmax(preds_for_ith_row)\n",
    "    \n",
    "def lgb_f1_micro(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    \n",
    "    num_data = len(y_true)\n",
    "    num_class = 3\n",
    "    \n",
    "    y_pred = []\n",
    "    for i in range(num_data):\n",
    "        ith_pred = get_ith_pred(preds, i, num_data, num_class)\n",
    "        y_pred.append(ith_pred)\n",
    "    \n",
    "    return 'f1', f1_score(y_true, y_pred, average='micro'), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-preference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "choice-system",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:59:23.309994Z",
     "start_time": "2021-03-09T11:58:18.560412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag: 0\n",
      "Bag: 1\n",
      "Bag: 2\n",
      "Bag: 3\n",
      "Bag: 4\n",
      "Bag: 5\n",
      "Bag: 6\n",
      "Bag: 7\n",
      "Bag: 8\n",
      "Bag: 9\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 120,\n",
    "         'min_child_samples': 40,\n",
    "         'learning_rate': 0.2,\n",
    "         'boosting_type': 'goss',\n",
    "         'objective': 'multiclass',\n",
    "         'num_class': 3,\n",
    "         'verbosity': -1,\n",
    "         'num_threads': 8,\n",
    "         'seed': 1}\n",
    "\n",
    "bags = 50\n",
    "bagged_prediction = np.zeros(X_test.shape[0])\n",
    "\n",
    "for n in range(bags):\n",
    "    print(f'Bag: {n}')\n",
    "    param['seed'] += n\n",
    "    booster = lgb.train(param,\n",
    "                        train_data,\n",
    "                        30,\n",
    "                        categorical_feature=list(cat_cols_plus_geo),\n",
    "                        feval=lgb_f1_micro)\n",
    "    \n",
    "    prob_preds = booster.predict(ct.transform(X_test))\n",
    "    preds = [np.argmax(p) + 1 for p in prob_preds]\n",
    "    bagged_prediction += preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-induction",
   "metadata": {},
   "source": [
    "How am I supposed to know if this model is better without testing it beforehand? Let's just make a model and submit it and see what the score is first and then worry about more submissions after. If I have a range of submissions I want to try, I can just build them up and submit new ones each day. I can even create a list of those I want to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "going-findings",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:59:26.957587Z",
     "start_time": "2021-03-09T11:59:26.947620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2,\n",
       "       2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3. ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(bagged_prediction / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sound-ecuador",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:59:29.955054Z",
     "start_time": "2021-03-09T11:59:29.950773Z"
    }
   },
   "outputs": [],
   "source": [
    "bagged_prediction /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "arabic-characterization",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T11:59:32.219488Z",
     "start_time": "2021-03-09T11:59:32.106157Z"
    }
   },
   "outputs": [],
   "source": [
    "rounded_preds = [int(round(pred)) for pred in bagged_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "seeing-omega",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T12:00:24.731322Z",
     "start_time": "2021-03-09T12:00:24.554073Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_format = pd.read_csv(DOWNLOAD_DIR / 'submission_format.csv',\n",
    "                         index_col='building_id')\n",
    "\n",
    "my_sub = pd.DataFrame(data=rounded_preds,\n",
    "                      columns=sub_format.columns,\n",
    "                      index=sub_format.index)\n",
    "\n",
    "title = '03-09 - LGBM API - lgbm_02_02 hyperparams - 10 bags, 30 rounds'\n",
    "\n",
    "my_sub.to_csv(SUBMISSIONS_DIR / f'{title}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-isaac",
   "metadata": {},
   "source": [
    "Woo jumped to 0.7469 \n",
    "From 228 to 167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bulgarian-essence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T12:12:21.027988Z",
     "start_time": "2021-03-09T12:12:21.010157Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_and_submit_bagged_lgbm(num_bags, title):\n",
    "    param = {'num_leaves': 120,\n",
    "         'min_child_samples': 40,\n",
    "         'learning_rate': 0.2,\n",
    "         'boosting_type': 'goss',\n",
    "         'objective': 'multiclass',\n",
    "         'num_class': 3,\n",
    "         'verbosity': -1,\n",
    "         'num_threads': 8,\n",
    "         'seed': 1}\n",
    "\n",
    "    bagged_prediction = np.zeros(X_test.shape[0])\n",
    "\n",
    "    for n in range(num_bags):\n",
    "        print(f'Bag: {n}')\n",
    "        param['seed'] += n\n",
    "        booster = lgb.train(param,\n",
    "                            train_data,\n",
    "                            30,\n",
    "                            categorical_feature=list(cat_cols_plus_geo),\n",
    "                            feval=lgb_f1_micro)\n",
    "\n",
    "        prob_preds = booster.predict(ct.transform(X_test))\n",
    "        preds = [np.argmax(p) + 1 for p in prob_preds]\n",
    "        bagged_prediction += preds\n",
    "        \n",
    "    bagged_prediction /= num_bags\n",
    "    rounded_preds = [int(round(pred)) for pred in bagged_prediction]\n",
    "    \n",
    "    sub_format = pd.read_csv(DOWNLOAD_DIR / 'submission_format.csv',\n",
    "                         index_col='building_id')\n",
    "\n",
    "    my_sub = pd.DataFrame(data=rounded_preds,\n",
    "                          columns=sub_format.columns,\n",
    "                          index=sub_format.index)\n",
    "\n",
    "    my_sub.to_csv(SUBMISSIONS_DIR / f'{title}.csv')\n",
    "    print('Submission created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "first-trainer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T12:17:18.873451Z",
     "start_time": "2021-03-09T12:12:56.882091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag: 0\n",
      "Bag: 1\n",
      "Bag: 2\n",
      "Bag: 3\n",
      "Bag: 4\n",
      "Bag: 5\n",
      "Bag: 6\n",
      "Bag: 7\n",
      "Bag: 8\n",
      "Bag: 9\n",
      "Bag: 10\n",
      "Bag: 11\n",
      "Bag: 12\n",
      "Bag: 13\n",
      "Bag: 14\n",
      "Bag: 15\n",
      "Bag: 16\n",
      "Bag: 17\n",
      "Bag: 18\n",
      "Bag: 19\n",
      "Bag: 20\n",
      "Bag: 21\n",
      "Bag: 22\n",
      "Bag: 23\n",
      "Bag: 24\n",
      "Bag: 25\n",
      "Bag: 26\n",
      "Bag: 27\n",
      "Bag: 28\n",
      "Bag: 29\n",
      "Bag: 30\n",
      "Bag: 31\n",
      "Bag: 32\n",
      "Bag: 33\n",
      "Bag: 34\n",
      "Bag: 35\n",
      "Bag: 36\n",
      "Bag: 37\n",
      "Bag: 38\n",
      "Bag: 39\n",
      "Bag: 40\n",
      "Bag: 41\n",
      "Bag: 42\n",
      "Bag: 43\n",
      "Bag: 44\n",
      "Bag: 45\n",
      "Bag: 46\n",
      "Bag: 47\n",
      "Bag: 48\n",
      "Bag: 49\n",
      "Submission created successfully!\n"
     ]
    }
   ],
   "source": [
    "title = '03-09 - LGBM API - lgbm_02_02 hyperparams - 50 bags, 30 rounds'\n",
    "build_and_submit_bagged_lgbm(50, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-intermediate",
   "metadata": {},
   "source": [
    "This got 0.7478!! From 168 to 132 (36 place jumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "metric-recommendation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T12:44:54.512907Z",
     "start_time": "2021-03-09T12:23:22.122467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag: 0\n",
      "Bag: 1\n",
      "Bag: 2\n",
      "Bag: 3\n",
      "Bag: 4\n",
      "Bag: 5\n",
      "Bag: 6\n",
      "Bag: 7\n",
      "Bag: 8\n",
      "Bag: 9\n",
      "Bag: 10\n",
      "Bag: 11\n",
      "Bag: 12\n",
      "Bag: 13\n",
      "Bag: 14\n",
      "Bag: 15\n",
      "Bag: 16\n",
      "Bag: 17\n",
      "Bag: 18\n",
      "Bag: 19\n",
      "Bag: 20\n",
      "Bag: 21\n",
      "Bag: 22\n",
      "Bag: 23\n",
      "Bag: 24\n",
      "Bag: 25\n",
      "Bag: 26\n",
      "Bag: 27\n",
      "Bag: 28\n",
      "Bag: 29\n",
      "Bag: 30\n",
      "Bag: 31\n",
      "Bag: 32\n",
      "Bag: 33\n",
      "Bag: 34\n",
      "Bag: 35\n",
      "Bag: 36\n",
      "Bag: 37\n",
      "Bag: 38\n",
      "Bag: 39\n",
      "Bag: 40\n",
      "Bag: 41\n",
      "Bag: 42\n",
      "Bag: 43\n",
      "Bag: 44\n",
      "Bag: 45\n",
      "Bag: 46\n",
      "Bag: 47\n",
      "Bag: 48\n",
      "Bag: 49\n",
      "Bag: 50\n",
      "Bag: 51\n",
      "Bag: 52\n",
      "Bag: 53\n",
      "Bag: 54\n",
      "Bag: 55\n",
      "Bag: 56\n",
      "Bag: 57\n",
      "Bag: 58\n",
      "Bag: 59\n",
      "Bag: 60\n",
      "Bag: 61\n",
      "Bag: 62\n",
      "Bag: 63\n",
      "Bag: 64\n",
      "Bag: 65\n",
      "Bag: 66\n",
      "Bag: 67\n",
      "Bag: 68\n",
      "Bag: 69\n",
      "Bag: 70\n",
      "Bag: 71\n",
      "Bag: 72\n",
      "Bag: 73\n",
      "Bag: 74\n",
      "Bag: 75\n",
      "Bag: 76\n",
      "Bag: 77\n",
      "Bag: 78\n",
      "Bag: 79\n",
      "Bag: 80\n",
      "Bag: 81\n",
      "Bag: 82\n",
      "Bag: 83\n",
      "Bag: 84\n",
      "Bag: 85\n",
      "Bag: 86\n",
      "Bag: 87\n",
      "Bag: 88\n",
      "Bag: 89\n",
      "Bag: 90\n",
      "Bag: 91\n",
      "Bag: 92\n",
      "Bag: 93\n",
      "Bag: 94\n",
      "Bag: 95\n",
      "Bag: 96\n",
      "Bag: 97\n",
      "Bag: 98\n",
      "Bag: 99\n",
      "Bag: 100\n",
      "Bag: 101\n",
      "Bag: 102\n",
      "Bag: 103\n",
      "Bag: 104\n",
      "Bag: 105\n",
      "Bag: 106\n",
      "Bag: 107\n",
      "Bag: 108\n",
      "Bag: 109\n",
      "Bag: 110\n",
      "Bag: 111\n",
      "Bag: 112\n",
      "Bag: 113\n",
      "Bag: 114\n",
      "Bag: 115\n",
      "Bag: 116\n",
      "Bag: 117\n",
      "Bag: 118\n",
      "Bag: 119\n",
      "Bag: 120\n",
      "Bag: 121\n",
      "Bag: 122\n",
      "Bag: 123\n",
      "Bag: 124\n",
      "Bag: 125\n",
      "Bag: 126\n",
      "Bag: 127\n",
      "Bag: 128\n",
      "Bag: 129\n",
      "Bag: 130\n",
      "Bag: 131\n",
      "Bag: 132\n",
      "Bag: 133\n",
      "Bag: 134\n",
      "Bag: 135\n",
      "Bag: 136\n",
      "Bag: 137\n",
      "Bag: 138\n",
      "Bag: 139\n",
      "Bag: 140\n",
      "Bag: 141\n",
      "Bag: 142\n",
      "Bag: 143\n",
      "Bag: 144\n",
      "Bag: 145\n",
      "Bag: 146\n",
      "Bag: 147\n",
      "Bag: 148\n",
      "Bag: 149\n",
      "Bag: 150\n",
      "Bag: 151\n",
      "Bag: 152\n",
      "Bag: 153\n",
      "Bag: 154\n",
      "Bag: 155\n",
      "Bag: 156\n",
      "Bag: 157\n",
      "Bag: 158\n",
      "Bag: 159\n",
      "Bag: 160\n",
      "Bag: 161\n",
      "Bag: 162\n",
      "Bag: 163\n",
      "Bag: 164\n",
      "Bag: 165\n",
      "Bag: 166\n",
      "Bag: 167\n",
      "Bag: 168\n",
      "Bag: 169\n",
      "Bag: 170\n",
      "Bag: 171\n",
      "Bag: 172\n",
      "Bag: 173\n",
      "Bag: 174\n",
      "Bag: 175\n",
      "Bag: 176\n",
      "Bag: 177\n",
      "Bag: 178\n",
      "Bag: 179\n",
      "Bag: 180\n",
      "Bag: 181\n",
      "Bag: 182\n",
      "Bag: 183\n",
      "Bag: 184\n",
      "Bag: 185\n",
      "Bag: 186\n",
      "Bag: 187\n",
      "Bag: 188\n",
      "Bag: 189\n",
      "Bag: 190\n",
      "Bag: 191\n",
      "Bag: 192\n",
      "Bag: 193\n",
      "Bag: 194\n",
      "Bag: 195\n",
      "Bag: 196\n",
      "Bag: 197\n",
      "Bag: 198\n",
      "Bag: 199\n",
      "Bag: 200\n",
      "Bag: 201\n",
      "Bag: 202\n",
      "Bag: 203\n",
      "Bag: 204\n",
      "Bag: 205\n",
      "Bag: 206\n",
      "Bag: 207\n",
      "Bag: 208\n",
      "Bag: 209\n",
      "Bag: 210\n",
      "Bag: 211\n",
      "Bag: 212\n",
      "Bag: 213\n",
      "Bag: 214\n",
      "Bag: 215\n",
      "Bag: 216\n",
      "Bag: 217\n",
      "Bag: 218\n",
      "Bag: 219\n",
      "Bag: 220\n",
      "Bag: 221\n",
      "Bag: 222\n",
      "Bag: 223\n",
      "Bag: 224\n",
      "Bag: 225\n",
      "Bag: 226\n",
      "Bag: 227\n",
      "Bag: 228\n",
      "Bag: 229\n",
      "Bag: 230\n",
      "Bag: 231\n",
      "Bag: 232\n",
      "Bag: 233\n",
      "Bag: 234\n",
      "Bag: 235\n",
      "Bag: 236\n",
      "Bag: 237\n",
      "Bag: 238\n",
      "Bag: 239\n",
      "Bag: 240\n",
      "Bag: 241\n",
      "Bag: 242\n",
      "Bag: 243\n",
      "Bag: 244\n",
      "Bag: 245\n",
      "Bag: 246\n",
      "Bag: 247\n",
      "Bag: 248\n",
      "Bag: 249\n",
      "Submission created successfully!\n"
     ]
    }
   ],
   "source": [
    "title = '03-09 - LGBM API - lgbm_02_02 hyperparams - 250 bags, 30 rounds'\n",
    "build_and_submit_bagged_lgbm(250, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-england",
   "metadata": {},
   "source": [
    "This got 0.7482\n",
    "Climbed from 132 --> 115 (17 place jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-reminder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
