diff --git a/analysis/bagging.ipynb b/analysis/bagging.ipynb
index f9628e4..459a7fc 100644
--- a/analysis/bagging.ipynb
+++ b/analysis/bagging.ipynb
@@ -50,12 +50,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 31,
    "id": "anonymous-savannah",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:07.314289Z",
-     "start_time": "2021-03-15T15:24:04.121919Z"
+     "end_time": "2021-03-16T16:21:53.349254Z",
+     "start_time": "2021-03-16T16:21:52.455303Z"
     },
     "executionInfo": {
      "elapsed": 2001,
@@ -81,6 +81,8 @@
     "import pickle\n",
     "from pathlib import Path\n",
     "from tqdm.notebook import trange, tqdm\n",
+    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
+    "from wandb.lightgbm import wandb_callback\n",
     "\n",
     "### USE FOR LOCAL JUPYTER NOTEBOOKS ###\n",
     "DOWNLOAD_DIR = Path('../download')\n",
@@ -106,12 +108,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 14,
    "id": "boxed-upper",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:07.512273Z",
-     "start_time": "2021-03-15T15:24:07.509191Z"
+     "end_time": "2021-03-16T15:09:47.403655Z",
+     "start_time": "2021-03-16T15:09:47.400451Z"
     }
    },
    "outputs": [],
@@ -544,8 +546,8 @@
    "id": "determined-library",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:13.810793Z",
-     "start_time": "2021-03-15T15:24:12.232620Z"
+     "end_time": "2021-03-16T14:54:55.919121Z",
+     "start_time": "2021-03-16T14:54:52.739507Z"
     },
     "id": "determined-library",
     "outputId": "420f632a-92fb-4c29-c91a-826eaa443af9",
@@ -621,12 +623,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 4,
    "id": "documentary-victoria",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:19.676054Z",
-     "start_time": "2021-03-15T15:24:19.671433Z"
+     "end_time": "2021-03-16T14:54:59.013301Z",
+     "start_time": "2021-03-16T14:54:59.009186Z"
     },
     "id": "documentary-victoria",
     "outputId": "023863ce-fc30-4800-bff8-17a5b793b0d8"
@@ -638,7 +640,7 @@
        "(86868, 38)"
       ]
      },
-     "execution_count": 5,
+     "execution_count": 4,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -649,12 +651,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 15,
    "id": "stock-companion",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:19.818633Z",
-     "start_time": "2021-03-15T15:24:19.804052Z"
+     "end_time": "2021-03-16T15:09:54.893621Z",
+     "start_time": "2021-03-16T15:09:54.890312Z"
     },
     "executionInfo": {
      "elapsed": 587,
@@ -682,12 +684,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 16,
    "id": "brief-karma",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:20.494575Z",
-     "start_time": "2021-03-15T15:24:19.962861Z"
+     "end_time": "2021-03-16T15:09:55.941871Z",
+     "start_time": "2021-03-16T15:09:55.485804Z"
     },
     "executionInfo": {
      "elapsed": 830,
@@ -710,12 +712,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 17,
    "id": "possible-neighbor",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:20.704068Z",
-     "start_time": "2021-03-15T15:24:20.663235Z"
+     "end_time": "2021-03-16T15:09:56.529575Z",
+     "start_time": "2021-03-16T15:09:56.490300Z"
     },
     "executionInfo": {
      "elapsed": 517,
@@ -741,12 +743,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 18,
    "id": "confidential-elimination",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:20.862876Z",
-     "start_time": "2021-03-15T15:24:20.859200Z"
+     "end_time": "2021-03-16T15:09:57.211712Z",
+     "start_time": "2021-03-16T15:09:57.209143Z"
     },
     "executionInfo": {
      "elapsed": 559,
@@ -769,12 +771,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 19,
    "id": "adaptive-dietary",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:23.530270Z",
-     "start_time": "2021-03-15T15:24:23.527237Z"
+     "end_time": "2021-03-16T15:09:58.481654Z",
+     "start_time": "2021-03-16T15:09:58.478934Z"
     },
     "executionInfo": {
      "elapsed": 414,
@@ -799,12 +801,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 20,
    "id": "judicial-error",
    "metadata": {
     "ExecuteTime": {
-     "end_time": "2021-03-15T15:24:24.063026Z",
-     "start_time": "2021-03-15T15:24:24.056677Z"
+     "end_time": "2021-03-16T15:09:59.804910Z",
+     "start_time": "2021-03-16T15:09:59.800581Z"
     },
     "executionInfo": {
      "elapsed": 569,
@@ -2083,7 +2085,7 @@
     "id": "addressed-discovery"
    },
    "source": [
-    "## Validating Bagged Models\n",
+    "## Bagged Models with WandB\n",
     "\n",
     "Let's build one model first and see how we can interact with it. I want to get some plots to see how we can use validation data. But using the LGBM API is SO MUCH BETTER than using the sklearn API for LightGBM, so let's play with this and see what happens."
    ]
@@ -2107,31 +2109,23 @@
     "\n",
     "full_train_data = lgb.Dataset(X_all_ints,\n",
     "                             label=y,\n",
-    "                             feature_name=list(cols_ordered_after_ordinal_encoding),\n",
-    "                             categorical_feature=list(cat_cols_plus_geo),\n",
+    "#                              feature_name=list(cols_ordered_after_ordinal_encoding),\n",
+    "#                              categorical_feature=list(cat_cols_plus_geo),\n",
     "                             free_raw_data=False)\n",
     "\n",
     "train_data = lgb.Dataset(X_train,\n",
     "                        label=y_train,\n",
-    "                        feature_name=list(cols_ordered_after_ordinal_encoding),\n",
-    "                        categorical_feature=list(cat_cols_plus_geo),\n",
+    "#                         feature_name=list(cols_ordered_after_ordinal_encoding),\n",
+    "#                         categorical_feature=list(cat_cols_plus_geo),\n",
     "                        free_raw_data=False)\n",
     "\n",
     "val_data = lgb.Dataset(X_val,\n",
     "                      label=y_val,\n",
-    "                      feature_name=list(cols_ordered_after_ordinal_encoding),\n",
-    "                      categorical_feature=list(cat_cols_plus_geo),\n",
+    "#                       feature_name=list(cols_ordered_after_ordinal_encoding),\n",
+    "#                       categorical_feature=list(cat_cols_plus_geo),\n",
     "                      free_raw_data=False)"
    ]
   },
-  {
-   "cell_type": "markdown",
-   "id": "devoted-pipeline",
-   "metadata": {},
-   "source": [
-    "### DO I NEED TO SET CATEGORICAL_FEATURE AND FEATURE_NAME IN LGB.TRAIN IF I HAVE SET THEM IN THE LGB.DATASET OBJECT?"
-   ]
-  },
   {
    "cell_type": "code",
    "execution_count": 62,
@@ -2306,7 +2300,7 @@
     "         'min_child_samples': 40,\n",
     "         'learning_rate': 0.03,\n",
     "         'num_boost_round': 40,\n",
-    "         'early_stopping_round': -1,\n",
+    "         'early_stopping_rounds': 12,\n",
     "         'boosting_type': 'goss',\n",
     "         'objective': 'multiclassova',\n",
     "         'is_unbalance': True,\n",
@@ -2390,8 +2384,6 @@
     }
    ],
    "source": [
-    "from wandb.lightgbm import wandb_callback\n",
-    "\n",
     "evals_result = {}\n",
     "booster = lgb.train(dict(wandb.config), # doesn't work if I pass wandb.config\n",
     "                    train_data,\n",
@@ -2403,6 +2395,7 @@
     "                    evals_result=evals_result,\n",
     "#                     early_stopping_rounds=None,\n",
     "                    categorical_feature=list(cat_cols_plus_geo),\n",
+    "                    feature_name=list(cols_ordered_after_ordinal_encoding)\n",
     "                    feval=lgb_f1_micro,\n",
     "                    callbacks=[wandb_callback()])"
    ]
@@ -2617,12 +2610,190 @@
    ]
   },
   {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "agricultural-longer",
+   "cell_type": "markdown",
+   "id": "ready-longitude",
    "metadata": {
     "id": "agricultural-longer"
    },
+   "source": [
+    "## Validating Bagged Models"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 30,
+   "id": "genuine-canyon",
+   "metadata": {
+    "ExecuteTime": {
+     "end_time": "2021-03-16T16:19:55.824807Z",
+     "start_time": "2021-03-16T16:19:55.819537Z"
+    },
+    "code_folding": []
+   },
+   "outputs": [],
+   "source": [
+    "def get_train_val_datasets(X, y, train_idx, val_idx):\n",
+    "    X_train, X_val = X[train_idx], X[val_idx]\n",
+    "    y_train, y_val = y[train_idx], y[val_idx]\n",
+    "    \n",
+    "    train_dataset = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
+    "    val_dataset = lgb.Dataset(X_val, label=y_val, free_raw_data=False)\n",
+    "    train_dataset.construct()\n",
+    "    val_dataset.construct()\n",
+    "    return train_dataset, val_dataset\n",
+    "\n",
+    "\n",
+    "def train_lgbm_model(config, train_dataset, val_dataset):\n",
+    "        evals_result = {}\n",
+    "        booster = lgb.train(config,\n",
+    "                           train_dataset,\n",
+    "                           valid_sets=[train_dataset, val_dataset],\n",
+    "                           valid_names=['train', 'val'],\n",
+    "                           evals_result=evals_result,\n",
+    "                           feval=lgb_f1_micro,\n",
+    "                           callbacks=[wandb_callback()])\n",
+    "        return booster, evals_result\n",
+    "\n",
+    "def eval_bagged_model(config, num_bags, train_dataset, val_dataset):\n",
+    "    bagged_preds = np.zeros(val_dataset.num_data())\n",
+    "    config = dict(config) # in case you input a wandb config object\n",
+    "    for n in range(num_bags):\n",
+    "        config['seed'] += n\n",
+    "        booster, evals_result = train_lgbm_model(config, train_dataset,\n",
+    "                                                val_dataset)\n",
+    "        # Do I need to predict? Does the callback do it for me automatically?\n",
+    "        pass"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 27,
+   "id": "responsible-circle",
+   "metadata": {
+    "ExecuteTime": {
+     "end_time": "2021-03-16T15:18:05.359253Z",
+     "start_time": "2021-03-16T15:18:03.407267Z"
+    }
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "<class 'numpy.ndarray'> 52121 (52121,)\n",
+      "52121\n",
+      "<class 'numpy.ndarray'> 52120 (52120,)\n",
+      "52120\n",
+      "<class 'numpy.ndarray'> 52120 (52120,)\n",
+      "52120\n",
+      "<class 'numpy.ndarray'> 52120 (52120,)\n",
+      "52120\n",
+      "<class 'numpy.ndarray'> 52120 (52120,)\n",
+      "52120\n"
+     ]
+    }
+   ],
+   "source": [
+    "param = {'num_leaves': 120,\n",
+    "         'min_child_samples': 40,\n",
+    "         'learning_rate': 0.03,\n",
+    "         'num_boost_round': 40,\n",
+    "         'early_stopping_rounds': 12,\n",
+    "         'boosting_type': 'goss',\n",
+    "         'objective': 'multiclassova',\n",
+    "         'is_unbalance': True,\n",
+    "         'metric': ['multiclassova', 'multi_error'],\n",
+    "         'num_class': 3,\n",
+    "         'verbosity': -1,\n",
+    "         'num_threads': 8,\n",
+    "         'seed': 1}\n",
+    "\n",
+    "run = wandb.init(project='earthquake_damage_competition',\n",
+    "                 config=param)\n",
+    "\n",
+    "skf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
+    "\n",
+    "all_eval_results = {}\n",
+    "# Cross-validation loop\n",
+    "for i, train_idx, val_idx in enumerate(skf.split(X_all_ints, y)):\n",
+    "    train_dataset, val_dataset = get_train_val_datasets(X_all_ints, y,\n",
+    "                                                       train_idx, val_idx)\n",
+    "    # Perform bagged model building and evaluation to get a score\n",
+    "    booster, evals_results = train_lgbm_model(param, train_dataset,\n",
+    "                                             val_dataset)\n",
+    "    \n",
+    "    "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "studied-patent",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "swiss-metadata",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "interested-dallas",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "inside-toilet",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "afraid-greensboro",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "objective-ownership",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "noble-platform",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "legal-polls",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "least-passport",
+   "metadata": {},
    "outputs": [],
    "source": []
   },
diff --git a/analysis/wandb/latest-run b/analysis/wandb/latest-run
index c3f2a32..0d29f0e 120000
--- a/analysis/wandb/latest-run
+++ b/analysis/wandb/latest-run
@@ -1 +1 @@
-run-20210315_175036-2yj128hh
\ No newline at end of file
+run-20210316_182522-1rmefeon
\ No newline at end of file
